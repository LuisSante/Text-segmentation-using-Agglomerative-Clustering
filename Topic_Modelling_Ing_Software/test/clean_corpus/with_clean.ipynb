{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "from bertopic import BERTopic\n",
    "from datetime import datetime\n",
    "from autocorrect import Speller #autocorrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura del dataset \"app_reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>package_name</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Great app! The new version now works on my Bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Great It's not fully optimised and has some is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Works on a Nexus 6p I'm still messing around w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>The bandwidth seemed to be limited to maximum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Works well with my Hackrf Hopefully new update...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288060</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>it doesn't do anything after installing this i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288061</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>I like this app . Its is very helpful for use....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288062</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>Finally Brings back the Unix command line to A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288063</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>The API feature is great  just need loads more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288064</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>Works Nicely! I wish there were instructions t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288065 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   package_name  \\\n",
       "0       com.mantz_it.rfanalyzer   \n",
       "1       com.mantz_it.rfanalyzer   \n",
       "2       com.mantz_it.rfanalyzer   \n",
       "3       com.mantz_it.rfanalyzer   \n",
       "4       com.mantz_it.rfanalyzer   \n",
       "...                         ...   \n",
       "288060           com.termux.api   \n",
       "288061           com.termux.api   \n",
       "288062           com.termux.api   \n",
       "288063           com.termux.api   \n",
       "288064           com.termux.api   \n",
       "\n",
       "                                                   review  \n",
       "0       Great app! The new version now works on my Bra...  \n",
       "1       Great It's not fully optimised and has some is...  \n",
       "2       Works on a Nexus 6p I'm still messing around w...  \n",
       "3       The bandwidth seemed to be limited to maximum ...  \n",
       "4       Works well with my Hackrf Hopefully new update...  \n",
       "...                                                   ...  \n",
       "288060  it doesn't do anything after installing this i...  \n",
       "288061  I like this app . Its is very helpful for use....  \n",
       "288062  Finally Brings back the Unix command line to A...  \n",
       "288063  The API feature is great  just need loads more...  \n",
       "288064  Works Nicely! I wish there were instructions t...  \n",
       "\n",
       "[288065 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = pd.read_csv('https://raw.githubusercontent.com/LuisSante/Datasets/main/app_reviews.csv')\n",
    "review = review.drop(['date','star'],axis=1)\n",
    "#review = pd.read_csv('C:/Users/USUARIO/Documents/Universidad/4A. Inteligencia Artificial/Dataset/app_reviews.csv')\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus_review):\n",
    "        \n",
    "    for i in range(len(corpus_review)):        \n",
    "        corpus_review[i] = re.sub(r'https?:\\/\\/.\\S+', \"\", corpus_review[i]) \n",
    "        corpus_review[i] = re.sub(r'\"', '', corpus_review[i]) \n",
    "        corpus_review[i] = re.sub(r'#', '', corpus_review[i]) \n",
    "        corpus_review[i] = re.sub(r'^RT[\\s]+', '', corpus_review[i])\n",
    "\n",
    "        Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'    ll\":\" will\", \n",
    "               \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"} #reemplazar apostrofes    \n",
    "          \n",
    "        for key,value in Apos_dict.items(): \n",
    "            if key in corpus_review[i]: \n",
    "                corpus_review[i]=corpus_review[i].replace(key,value) #reemplazar\n",
    "\n",
    "        corpus_review[i] = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",corpus_review[i]) if s])\n",
    "        corpus_review[i]=corpus_review[i].lower() #minuscula\n",
    "\n",
    "        file=open(\"slang.txt\",\"r\") #jergas del ingles\n",
    "        slang=file.read() \n",
    "          \n",
    "        slang=slang.split('\\n') \n",
    "          \n",
    "        tweet_tokens= corpus_review[i].split() \n",
    "        slang_word=[] \n",
    "        meaning=[] \n",
    "          \n",
    "        for line in slang: \n",
    "            temp=line.split(\"=\") \n",
    "            slang_word.append(temp[0]) \n",
    "            meaning.append(temp[-1]) \n",
    "          \n",
    "        for i,word in enumerate(tweet_tokens): \n",
    "            if word in slang_word: \n",
    "                idx=slang_word.index(word) \n",
    "                tweet_tokens[i]=meaning[idx]\n",
    "        \n",
    "        corpus_review[i]=\" \".join(tweet_tokens) \n",
    "        corpus_review[i] = ''.join(''.join(s)[:2] for _, s in itertools.groupby(corpus_review[i]))   \n",
    " \n",
    "        spell = Speller(lang='en') \n",
    "        corpus_review[i]=spell(corpus_review[i]) \n",
    "    return corpus_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se extrae en un corpus todos los reviews o criticas de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus(dataset):\n",
    "    print(datetime.today(), \"extrayendo oraciones...\")\n",
    "    lista = []  \n",
    "    for i in range(len(dataset['package_name'].unique())):#iterar entre los package_name unicos\n",
    "        dataset_temp = dataset.loc[dataset['package_name'] == dataset['package_name'].unique()[i]]\n",
    "        lista.append({'package_name':dataset['package_name'].unique()[i], 'size': len(dataset_temp)})#otener un package_name y el n√∫mero de oraciones\n",
    "\n",
    "    lista = sorted(lista, key=lambda x: x['size'], reverse=True)#se ordena para saber que package_name tiene el mayor n¬∞ de oraciones\n",
    "    dataframe = dataset[dataset['package_name'] == lista[8]['package_name']]#el mayor ser√° el elemnto que ocupa la posicion 0\n",
    "    corpus = list(dataframe['review'])#extraemos un corpus\n",
    "    \n",
    "    print(datetime.today(), \"limpiando el corpus...\")\n",
    "    corpus = clean_corpus(corpus)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir datos en un Dataframe a un manejo m√°s √°gil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_corpus_to_dataFrame(corpus):\n",
    "    print(datetime.today(), \"Convirtiendo las oraciones extraidas a un dataframe...\")\n",
    "    corpus_ds = {\n",
    "        'Sentences' : corpus\n",
    "    }\n",
    "\n",
    "    dataset_new = pd.DataFrame(corpus_ds)\n",
    "    return dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea una funci√≥n que nos permita incrustar las oraciones, para esto usamos un modelo pre-entrenado de SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_embeddings(dataset):\n",
    "    model_embedder = SentenceTransformer('all-MiniLM-L6-v2')#modelo pre-entrenado\n",
    "    corpus = extract_corpus(dataset)#extraemos un corpus del dataset \n",
    "    print(datetime.today(), \"Incrustando las oraciones...\")\n",
    "    embeddings = model_embedder.encode(corpus, \n",
    "                                        convert_to_tensor=False, \n",
    "                                        show_progress_bar=True) #generamos las incrustaciones \n",
    "\n",
    "    embeddings = embeddings /  np.linalg.norm(embeddings, axis=1, keepdims=True) #normalizamos\n",
    "\n",
    "    return embeddings, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para obtener el \"mejor\" cluster aplicamos el m√©todo de la silueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en esta funcion hace la tarea de obtener el mejor k con agglomerative clustering\n",
    "def silhoutte(dataset, attempts):\n",
    "    embeddings, corpus = neural_embeddings(dataset)\n",
    "    print(datetime.today(), \"Calculando el mejor k...\")\n",
    "    scores_silhouette = [] #guardaremos todos los resultados del m√©todo de la silueta para devolver el mayor\n",
    "\n",
    "    for k in range(2,attempts+1):\n",
    "        agglomerative_clusterering = AgglomerativeClustering(n_clusters=k, \n",
    "                                                            affinity=\"cosine\" , \n",
    "                                                            linkage=\"complete\").fit(embeddings)\n",
    "                                                            \n",
    "        cluster_labels = agglomerative_clusterering.labels_\n",
    "\n",
    "        silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
    "        scores_silhouette.append(silhouette_avg)\n",
    "\n",
    "    max_score = max(scores_silhouette)\n",
    "    max_index = scores_silhouette.index(max_score)\n",
    "    n_clusters = max_index + 2\n",
    "\n",
    "    return n_clusters, embeddings, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Al tener el \"mejor\" n√∫mero de clusters, se procede a segmentar las oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_segmentation(dataset_review, attempts):\n",
    "    n_clusters, embeddings, corpus = silhoutte(dataset_review, attempts) # se le pasa el mejor K\n",
    "\n",
    "    agglomerative_clusterering = AgglomerativeClustering(n_clusters=n_clusters, \n",
    "                                                        affinity=\"cosine\", \n",
    "                                                        linkage=\"complete\").fit(embeddings)\n",
    "                                                        \n",
    "    cluster_labels = agglomerative_clusterering.labels_ #obtengo las etiquetas respectivas a las oraciones\n",
    "\n",
    "    \n",
    "    dataframe = convert_corpus_to_dataFrame(corpus)\n",
    "    dataframe['cluster'] = cluster_labels\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-22 10:41:01.721964 extrayendo oraciones...\n",
      "2022-08-22 10:41:38.156747 limpiando el corpus...\n",
      "2022-08-22 10:55:48.991027 Incrustando las oraciones...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389b2345b3404367af9cb975dece495f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-22 10:57:59.346241 Calculando el mejor k...\n",
      "2022-08-22 11:00:49.710302 Convirtiendo las oraciones extraidas a un dataframe...\n"
     ]
    }
   ],
   "source": [
    "data = topics_segmentation(review, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>veryold</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ÿπÿßŸÑ€å good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the best ever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>awesome. love it üòúüòúüòÅüòÅüòúüòÅüòÅ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling very well</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>i am feeling very well</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>does not work  ca not add any accounts.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>only one concern but not sure if it was the ap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>verygood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>this is very bad and not change password and v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2976 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentences  cluster\n",
       "0                                               veryold        1\n",
       "1                                             ÿπÿßŸÑ€å good        1\n",
       "2                                         the best ever        1\n",
       "3                              awesome. love it üòúüòúüòÅüòÅüòúüòÅüòÅ        1\n",
       "4                                i am feeling very well        1\n",
       "...                                                 ...      ...\n",
       "2971                             i am feeling very well        1\n",
       "2972            does not work  ca not add any accounts.        1\n",
       "2973  only one concern but not sure if it was the ap...        0\n",
       "2974                                           verygood        1\n",
       "2975  this is very bad and not change password and v...        0\n",
       "\n",
       "[2976 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cluster'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110fe3fb9777db4ce1f884af3cc527a40b2c98427ad17781c021ef692bd3d28d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
