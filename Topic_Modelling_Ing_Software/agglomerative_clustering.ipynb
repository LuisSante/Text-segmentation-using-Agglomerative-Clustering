{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from bertopic import BERTopic\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import contractions\n",
    "import inflect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura del dataset \"app_reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>why I can't log in my account!?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can't turn in my activities. Please fix this i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Never shows class work and I have to use the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After the recent update, every time I logged i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Files do not attached</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>I dont like this app. Too much activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Ga bisa buka video dari guru gajelas nih app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Graveyard of students.... RIP students.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>worst app i have ever seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Crashed and not loading 游땞游땞游땞</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows 칑 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences\n",
       "0                       why I can't log in my account!?\n",
       "1     Can't turn in my activities. Please fix this i...\n",
       "2     Never shows class work and I have to use the w...\n",
       "3     After the recent update, every time I logged i...\n",
       "4                                 Files do not attached\n",
       "...                                                 ...\n",
       "9995          I dont like this app. Too much activities\n",
       "9996       Ga bisa buka video dari guru gajelas nih app\n",
       "9997            Graveyard of students.... RIP students.\n",
       "9998                         worst app i have ever seen\n",
       "9999                        Crashed and not loading 游땞游땞游땞\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = pd.read_csv('C:/Users/Usuario/Desktop/Text-segmentation-using-Agglomerative-Clustering/extract_dataset/dataset_classroom.csv')\n",
    "#review = pd.read_csv('C:/Users/Usuario/Desktop/Text-segmentation-using-Agglomerative-Clustering/extract_dataset/dataset_classroom2.csv')\n",
    "review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacemos un pre procesamiento de las oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = replace_contractions(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-27 03:06:24.040764 Preprocesando los datos...\n"
     ]
    }
   ],
   "source": [
    "print(datetime.today(), \"Preprocesando los datos...\")\n",
    "\n",
    "review['sentences'] = review['sentences'].astype(str)\n",
    "for i in range(len(review['sentences'])):\n",
    "    review['sentences'][i] = denoise_text(review['sentences'][i])\n",
    "    review['sentences'][i] = nltk.word_tokenize(review['sentences'][i])\n",
    "    review['sentences'][i] = normalize(review['sentences'][i])\n",
    "\n",
    "review['sentences'] = review['sentences'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['log', 'account']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['turn', 'activities', 'please', 'fix', 'immed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['never', 'shows', 'class', 'work', 'use', 'we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['recent', 'update', 'every', 'time', 'logged'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['files', 'attached']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>['like', 'app', 'much', 'activities']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>['ga', 'bisa', 'buka', 'video', 'dari', 'guru'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>['graveyard', 'students', 'rip', 'students']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>['worst', 'app', 'ever', 'seen']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>['crashed', 'loading']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows 칑 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences\n",
       "0                                    ['log', 'account']\n",
       "1     ['turn', 'activities', 'please', 'fix', 'immed...\n",
       "2     ['never', 'shows', 'class', 'work', 'use', 'we...\n",
       "3     ['recent', 'update', 'every', 'time', 'logged'...\n",
       "4                                 ['files', 'attached']\n",
       "...                                                 ...\n",
       "9995              ['like', 'app', 'much', 'activities']\n",
       "9996  ['ga', 'bisa', 'buka', 'video', 'dari', 'guru'...\n",
       "9997       ['graveyard', 'students', 'rip', 'students']\n",
       "9998                   ['worst', 'app', 'ever', 'seen']\n",
       "9999                             ['crashed', 'loading']\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se extrae en un corpus todos los reviews o criticas de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus(dataset):\n",
    "    corpus = dataset['sentences']\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir datos en un Dataframe a un manejo m치s 치gil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_corpus_to_dataFrame(corpus):\n",
    "    print(datetime.today(), \"Convirtiendo las oraciones extraidas a un dataframe...\")\n",
    "    corpus_ds = {\n",
    "        'Sentences' : corpus\n",
    "    }\n",
    "\n",
    "    dataset_new = pd.DataFrame(corpus_ds)\n",
    "    return dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_embbedings_to_dataFrame(embeddings):\n",
    "    print(datetime.today(), \"Convirtiendo las incrustaciones a un dataframe...\")\n",
    "    array = []\n",
    "    for i in embeddings:\n",
    "        array.append([i])\n",
    "\n",
    "    dataset_new = pd.DataFrame(array, columns=['Embeddings'])\n",
    "    return dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea una funci칩n que nos permita incrustar las oraciones, para esto usamos un modelo pre-entrenado de SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_embeddings(dataset):\n",
    "    model_embedder = SentenceTransformer('all-MiniLM-L6-v2')#modelo pre-entrenado\n",
    "    corpus = extract_corpus(dataset)#extraemos un corpus del dataset \n",
    "    print(datetime.today(), \"Incrustando las oraciones...\")\n",
    "    embeddings = model_embedder.encode(corpus, \n",
    "                                        convert_to_tensor=False, \n",
    "                                        show_progress_bar=True) #generamos las incrustaciones \n",
    "\n",
    "    embeddings = embeddings /  np.linalg.norm(embeddings, axis=1, keepdims=True) #normalizamos\n",
    "\n",
    "    return embeddings, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_embeddings_queries(queries):\n",
    "    model_embedder = SentenceTransformer('all-MiniLM-L6-v2')#modelo pre-entrenado\n",
    "\n",
    "    embeddings_queries = model_embedder.encode(queries, \n",
    "                                        convert_to_tensor=False) #generamos las incrustaciones \n",
    "\n",
    "    embeddings_queries = embeddings_queries /  np.linalg.norm(embeddings_queries, axis=0, keepdims=True) #normalizamos\n",
    "\n",
    "    return embeddings_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para obtener el \"mejor\" cluster aplicamos el m칠todo de la silueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en esta funcion hace la tarea de obtener el mejor k con agglomerative clustering\n",
    "def silhoutte(dataset, attempts):\n",
    "    embeddings, corpus = neural_embeddings(dataset)\n",
    "    print(datetime.today(), \"Calculando el mejor k...\")\n",
    "    scores_silhouette = [] #guardaremos todos los resultados del m칠todo de la silueta para devolver el mayor\n",
    "\n",
    "    for k in range(2,attempts+1):\n",
    "        agglomerative_clusterering = AgglomerativeClustering(n_clusters=k, \n",
    "                                                            affinity=\"cosine\" , \n",
    "                                                            linkage=\"complete\").fit(embeddings)\n",
    "                                                            \n",
    "        cluster_labels = agglomerative_clusterering.labels_\n",
    "\n",
    "        silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
    "        scores_silhouette.append(silhouette_avg)\n",
    "\n",
    "    max_score = max(scores_silhouette)\n",
    "    max_index = scores_silhouette.index(max_score)\n",
    "    n_clusters = max_index + 2\n",
    "\n",
    "    return n_clusters, embeddings, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Al tener el \"mejor\" n칰mero de clusters, se procede a segmentar las oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_segmentation(dataset_review, attempts):\n",
    "    n_clusters, embeddings, corpus = silhoutte(dataset_review, attempts) # se le pasa el mejor K\n",
    "\n",
    "    agglomerative_clusterering = AgglomerativeClustering(n_clusters=n_clusters, \n",
    "                                                        affinity=\"cosine\", \n",
    "                                                        linkage=\"complete\").fit(embeddings)\n",
    "                                                        \n",
    "    cluster_labels = agglomerative_clusterering.labels_ #obtengo las etiquetas respectivas a las oraciones\n",
    "\n",
    "    model_topics = BERTopic(nr_topics = n_clusters, language='english') # entreno para sacar K temas \n",
    "    topics, prob = model_topics.fit_transform(corpus)\n",
    "\n",
    "    label_topics = model_topics.generate_topic_labels(nr_words=5, topic_prefix=False) # temas\n",
    "    label_topics.pop(0) #elimino el grupo de temas at칤picos\n",
    "\n",
    "    return cluster_labels, label_topics, embeddings, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A cada oraci칩n le asignamos el cluster al que pertenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(dataset_review, attempts):\n",
    "    cluster_labels, label_topics, embeddings, corpus = topics_segmentation(dataset_review, attempts)\n",
    "    print(datetime.today(), \"Asignando un cluster a cada oraci칩n...\")\n",
    "    corpus_dataframe = convert_corpus_to_dataFrame(corpus) #de set de oraciones se convierte en un DF para asignarle su n칰mero de cluster\n",
    "    corpus_dataframe['cluster'] = cluster_labels #se le asigna a cada oraci칩n un cluster\n",
    "\n",
    "    return embeddings, label_topics, corpus_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B칰squeda sem치ntica para encontrar el tema de cada cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(dataset_review, attemps):\n",
    "    embeddings, label_topics, corpus_dataframe = clustering(dataset_review, attemps) #tomo embeddings para no volver a hacer el mismo trabajo 2 veces\n",
    "    dataframe_embeddings = convert_embbedings_to_dataFrame(embeddings) #convierto a cada lista de embeddings en un DF para evaluar con cada tema\n",
    "    dataframe_embeddings['cluster'] = corpus_dataframe['cluster'] # le asigno los clusters\n",
    "    sort_embeddings =  dataframe_embeddings.sort_values(by=['cluster']) \n",
    "    sort_embeddings = sort_embeddings.reset_index(drop=True)\n",
    "    nr_clusters = sort_embeddings['cluster'].unique() # extrae las un representante de cada cluster\n",
    "\n",
    "    first_sentences = [] #se almacenara cada primera oracion incrustada de cada cluster para asignarle un topic\n",
    "    j = 0\n",
    "    i = 0\n",
    "    while i < len(sort_embeddings):               \n",
    "        if(j < len(nr_clusters) and sort_embeddings['cluster'][i] == nr_clusters[j]):\n",
    "            first_sentences.append(sort_embeddings['Embeddings'][i]) #almacena\n",
    "            j+=1\n",
    "        i+=1\n",
    "\n",
    "    queries = label_topics #queries seran los temas\n",
    "    topics = [] \n",
    "    in_clusters = [] #se almacena los temas y los clusters, para que tengan un mismo 칤ndice en com칰n\n",
    "    print(datetime.today(), \"Incrustando los temas...\")\n",
    "    for topic in queries:\n",
    "\n",
    "        embeddings_queries = neural_embeddings_queries(topic) #incrusta los temas \n",
    "        cos_scores = util.cos_sim(embeddings_queries, first_sentences)[0] #se saca la similaridad de cada tema con respecto a las demas oraciones\n",
    "\n",
    "        cos_scores_numpy = cos_scores.numpy() #se convierte a tensor a numpu\n",
    "        cos_scores_list = cos_scores_numpy.tolist() #se convierte de numpy a list\n",
    "        max_coincidence = max(cos_scores_list)\n",
    "        cluster = cos_scores_list.index(max_coincidence)\n",
    "\n",
    "        if(len(topics) == 0): \n",
    "            topics.append(topic)\n",
    "            in_clusters.append(cluster)\n",
    "\n",
    "        elif(topic not in topics and cluster not in in_clusters): #para que no repite un tema con un cluster y viceversa\n",
    "            topics.append(topic)\n",
    "            in_clusters.append(cluster)\n",
    "    tupla = [] ##tama침o k -> k es el tama침o de cluster\n",
    "    for i in range(len(topics)):\n",
    "        tupla.append({'Topics': topics[i] , 'Cluster': in_clusters[i]}) #empareja    \n",
    "\n",
    "    return embeddings, tupla, corpus_dataframe\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostraremos un gr치fico de la segmentaci칩n de oraciones y un DataFrame de las oraciones con su respectivo cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_themes(dataset_review, attemps):\n",
    "    embeddings, reporte_tuplas, corpus_dataframe = semantic_search(dataset_review, attemps)\n",
    "    \n",
    "    print(datetime.today(), \"obteniendo temas...\")\n",
    "\n",
    "    reporte_tuplas = pd.DataFrame(reporte_tuplas, columns=['Topics','Cluster'])\n",
    "    reporte_tuplas = reporte_tuplas.sort_values(by=['Cluster'])\n",
    "    reporte_tuplas = reporte_tuplas.reset_index(drop=True)\n",
    "\n",
    "    assign = [] \n",
    "    lista_reporte = list(reporte_tuplas['Cluster'])\n",
    "    lista_topics = list(corpus_dataframe['cluster'])\n",
    "    for i in range(len(corpus_dataframe)):\n",
    "        if(lista_topics[i] in lista_reporte):\n",
    "            indexs = lista_reporte.index(lista_topics[i])\n",
    "            assign.append(reporte_tuplas['Topics'][indexs])\n",
    "        else:\n",
    "            assign.append(\"-1\")\n",
    "    \n",
    "    corpus_dataframe['Topics'] = assign\n",
    "    \n",
    "    return corpus_dataframe, reporte_tuplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-27 03:06:42.116074 Incrustando las oraciones...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1432ee911f814ea4a4daf72253691844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-27 03:07:56.136132 Calculando el mejor k...\n",
      "2022-12-27 03:59:03.054457 Asignando un cluster a cada oraci칩n...\n",
      "2022-12-27 03:59:03.055353 Convirtiendo las oraciones extraidas a un dataframe...\n",
      "2022-12-27 03:59:03.057106 Convirtiendo las incrustaciones a un dataframe...\n",
      "2022-12-27 03:59:03.193682 Incrustando los temas...\n",
      "2022-12-27 03:59:52.009865 obteniendo temas...\n"
     ]
    }
   ],
   "source": [
    "dataframe, reporte_tuplas = show_themes(review, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['log', 'account']</td>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['turn', 'activities', 'please', 'fix', 'immed...</td>\n",
       "      <td>90</td>\n",
       "      <td>super_complicated_difficult_hard_use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['never', 'shows', 'class', 'work', 'use', 'we...</td>\n",
       "      <td>88</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['recent', 'update', 'every', 'time', 'logged'...</td>\n",
       "      <td>3</td>\n",
       "      <td>account_switch_login_booo_log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['files', 'attached']</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>['like', 'app', 'much', 'activities']</td>\n",
       "      <td>65</td>\n",
       "      <td>bad_app_veryyyyyyyyyyyy_comment_lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>['ga', 'bisa', 'buka', 'video', 'dari', 'guru'...</td>\n",
       "      <td>59</td>\n",
       "      <td>hate_app_window_suck_school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>['graveyard', 'students', 'rip', 'students']</td>\n",
       "      <td>16</td>\n",
       "      <td>hate_school_hates_love_despise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>['worst', 'app', 'ever', 'seen']</td>\n",
       "      <td>59</td>\n",
       "      <td>hate_app_window_suck_school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>['crashed', 'loading']</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows 칑 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentences  cluster  \\\n",
       "0                                    ['log', 'account']       37   \n",
       "1     ['turn', 'activities', 'please', 'fix', 'immed...       90   \n",
       "2     ['never', 'shows', 'class', 'work', 'use', 'we...       88   \n",
       "3     ['recent', 'update', 'every', 'time', 'logged'...        3   \n",
       "4                                 ['files', 'attached']       18   \n",
       "...                                                 ...      ...   \n",
       "9995              ['like', 'app', 'much', 'activities']       65   \n",
       "9996  ['ga', 'bisa', 'buka', 'video', 'dari', 'guru'...       59   \n",
       "9997       ['graveyard', 'students', 'rip', 'students']       16   \n",
       "9998                   ['worst', 'app', 'ever', 'seen']       59   \n",
       "9999                             ['crashed', 'loading']       25   \n",
       "\n",
       "                                    Topics  \n",
       "0                                       -1  \n",
       "1     super_complicated_difficult_hard_use  \n",
       "2                                       -1  \n",
       "3            account_switch_login_booo_log  \n",
       "4                                       -1  \n",
       "...                                    ...  \n",
       "9995   bad_app_veryyyyyyyyyyyy_comment_lol  \n",
       "9996           hate_app_window_suck_school  \n",
       "9997        hate_school_hates_love_despise  \n",
       "9998           hate_app_window_suck_school  \n",
       "9999                                    -1  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topics</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google_classroom_lecture_access_went</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>account_switch_login_booo_log</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gg_baf_bleh_sucs_glichy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>balls_dog_dogshit_water_hamburger</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>school_related_work_schoolwork_made</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ayoko_aral_mag_na_ko</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>classroom_google_problem_assignments_app</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ew_eww_wew_screwww_luwhhhh</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dark_mode_racist_theme_darkmode</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>submit_know_assignments_assignment_work</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hate_cringe_hated_thank_nice</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hate_school_hates_love_despise</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>horrible_disgusting_disappointed_rape_wicked</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>na_hindi_ng_ang_apps</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bad_haahahah_badness_dangerous_badly</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ng_na_yung_mga_sa</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sucks_school_online_lol_stand</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>made_die_cry_died_make</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>galaxy_work_mobile_computer_phone</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>____</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>depression_gave_gives_cancer_pain</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>genshin_anniversary_rewards_2nd_impact</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>upload_takes_long_uploading_slow</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>que_de_es_la_el</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>updated_uninstall_update_still_fix</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>school_h8_enjoy_want_no1</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nice_cool_dope_awesome_best</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>online_students_useful_helpful_useless</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>deosnt_um_worst_language_teachers</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sono_gioco_non_le_nu</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>laggy_lagging_lag_slow_ughhh</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>trash_store_dustbin_packing_yeahhhh</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>hate_app_window_suck_school</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>work_dose_want_ahhhhhhhh_doesent</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gay_makes_kms_apes_made</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bad_app_veryyyyyyyyyyyy_comment_lol</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>terrible_bore_describe_mediocre_thrash</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>assignments_assignment_list_many_2many</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>good_fantastic_amazing_excellent_bahaha</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>crashes_crashing_keeps_crash_constantly</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>files_open_drive_offline_save</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>open_files_opening_file_unable</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>super_complicated_difficult_hard_use</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tatti_tati_bostaaaaaa_nomodulesonjanuary3_tngi...</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>like_likey_apppppp_thanyou_cllas</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>notification_notifications_notify_late_receiving</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Topics  Cluster\n",
       "0                google_classroom_lecture_access_went        2\n",
       "1                       account_switch_login_booo_log        3\n",
       "2                             gg_baf_bleh_sucs_glichy        4\n",
       "3                   balls_dog_dogshit_water_hamburger        5\n",
       "4                 school_related_work_schoolwork_made        6\n",
       "5                                ayoko_aral_mag_na_ko        7\n",
       "6            classroom_google_problem_assignments_app        8\n",
       "7                          ew_eww_wew_screwww_luwhhhh        9\n",
       "8                     dark_mode_racist_theme_darkmode       10\n",
       "9             submit_know_assignments_assignment_work       13\n",
       "10                       hate_cringe_hated_thank_nice       14\n",
       "11                     hate_school_hates_love_despise       16\n",
       "12       horrible_disgusting_disappointed_rape_wicked       19\n",
       "13                               na_hindi_ng_ang_apps       20\n",
       "14               bad_haahahah_badness_dangerous_badly       21\n",
       "15                                  ng_na_yung_mga_sa       27\n",
       "16                      sucks_school_online_lol_stand       29\n",
       "17                             made_die_cry_died_make       30\n",
       "18                  galaxy_work_mobile_computer_phone       31\n",
       "19                                               ____       33\n",
       "20                  depression_gave_gives_cancer_pain       36\n",
       "21             genshin_anniversary_rewards_2nd_impact       39\n",
       "22                   upload_takes_long_uploading_slow       40\n",
       "23                                    que_de_es_la_el       41\n",
       "24                 updated_uninstall_update_still_fix       42\n",
       "25                           school_h8_enjoy_want_no1       43\n",
       "26                        nice_cool_dope_awesome_best       45\n",
       "27             online_students_useful_helpful_useless       46\n",
       "28                  deosnt_um_worst_language_teachers       49\n",
       "29                               sono_gioco_non_le_nu       50\n",
       "30                       laggy_lagging_lag_slow_ughhh       54\n",
       "31                trash_store_dustbin_packing_yeahhhh       57\n",
       "32                        hate_app_window_suck_school       59\n",
       "33                   work_dose_want_ahhhhhhhh_doesent       60\n",
       "34                            gay_makes_kms_apes_made       62\n",
       "35                bad_app_veryyyyyyyyyyyy_comment_lol       65\n",
       "36             terrible_bore_describe_mediocre_thrash       66\n",
       "37             assignments_assignment_list_many_2many       67\n",
       "38            good_fantastic_amazing_excellent_bahaha       81\n",
       "39            crashes_crashing_keeps_crash_constantly       82\n",
       "40                      files_open_drive_offline_save       83\n",
       "41                     open_files_opening_file_unable       89\n",
       "42               super_complicated_difficult_hard_use       90\n",
       "43  tatti_tati_bostaaaaaa_nomodulesonjanuary3_tngi...       91\n",
       "44                   like_likey_apppppp_thanyou_cllas       94\n",
       "45   notification_notifications_notify_late_receiving       95"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reporte_tuplas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c63d8c7d738c2960218a10995aedf0a7f67a49a231e71037adf0440953cdb45b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
