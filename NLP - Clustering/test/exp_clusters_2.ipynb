{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import umap\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from autocorrect import Speller #autocorrector\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22.4\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>package_name</th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Great app! The new version now works on my Bra...</td>\n",
       "      <td>October 12 2016</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Great It's not fully optimised and has some is...</td>\n",
       "      <td>August 23 2016</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Works on a Nexus 6p I'm still messing around w...</td>\n",
       "      <td>August 04 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>The bandwidth seemed to be limited to maximum ...</td>\n",
       "      <td>July 25 2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Works well with my Hackrf Hopefully new update...</td>\n",
       "      <td>July 22 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288060</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>it doesn't do anything after installing this i...</td>\n",
       "      <td>June 24 2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288061</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>I like this app . Its is very helpful for use....</td>\n",
       "      <td>June 20 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288062</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>Finally Brings back the Unix command line to A...</td>\n",
       "      <td>May 20 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288063</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>The API feature is great  just need loads more...</td>\n",
       "      <td>May 05 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288064</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>Works Nicely! I wish there were instructions t...</td>\n",
       "      <td>April 28 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288065 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   package_name  \\\n",
       "0       com.mantz_it.rfanalyzer   \n",
       "1       com.mantz_it.rfanalyzer   \n",
       "2       com.mantz_it.rfanalyzer   \n",
       "3       com.mantz_it.rfanalyzer   \n",
       "4       com.mantz_it.rfanalyzer   \n",
       "...                         ...   \n",
       "288060           com.termux.api   \n",
       "288061           com.termux.api   \n",
       "288062           com.termux.api   \n",
       "288063           com.termux.api   \n",
       "288064           com.termux.api   \n",
       "\n",
       "                                                   review             date  \\\n",
       "0       Great app! The new version now works on my Bra...  October 12 2016   \n",
       "1       Great It's not fully optimised and has some is...   August 23 2016   \n",
       "2       Works on a Nexus 6p I'm still messing around w...   August 04 2016   \n",
       "3       The bandwidth seemed to be limited to maximum ...     July 25 2016   \n",
       "4       Works well with my Hackrf Hopefully new update...     July 22 2016   \n",
       "...                                                   ...              ...   \n",
       "288060  it doesn't do anything after installing this i...     June 24 2016   \n",
       "288061  I like this app . Its is very helpful for use....     June 20 2016   \n",
       "288062  Finally Brings back the Unix command line to A...      May 20 2016   \n",
       "288063  The API feature is great  just need loads more...      May 05 2016   \n",
       "288064  Works Nicely! I wish there were instructions t...    April 28 2016   \n",
       "\n",
       "        star  \n",
       "0          4  \n",
       "1          4  \n",
       "2          5  \n",
       "3          3  \n",
       "4          5  \n",
       "...      ...  \n",
       "288060     3  \n",
       "288061     5  \n",
       "288062     5  \n",
       "288063     5  \n",
       "288064     5  \n",
       "\n",
       "[288065 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = pd.read_csv('https://raw.githubusercontent.com/LuisSante/Datasets/main/app_reviews.csv')\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus_review):\n",
    "        \n",
    "    for i in range(len(corpus_review)):        \n",
    "        corpus_review[i] = re.sub(r'https?:\\/\\/.\\S+', \"\", corpus_review[i]) \n",
    "        corpus_review[i] = re.sub(r'\"', '', corpus_review[i]) \n",
    "        corpus_review[i] = re.sub(r'#', '', corpus_review[i]) \n",
    "        corpus_review[i] = re.sub(r'^RT[\\s]+', '', corpus_review[i]) \n",
    "              \n",
    "        Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'    ll\":\" will\", \n",
    "               \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"} #reemplazar apostrofes    \n",
    "          \n",
    "        for key,value in Apos_dict.items(): \n",
    "            if key in corpus_review[i]: \n",
    "                corpus_review[i]=corpus_review[i].replace(key,value) #reemplazar\n",
    "\n",
    "        corpus_review[i] = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",corpus_review[i]) if s])\n",
    "        corpus_review[i]=corpus_review[i].lower() #minusculas\n",
    "\n",
    "        file=open(\"slang.txt\",\"r\") #jergas del ingles\n",
    "        slang=file.read() \n",
    "          \n",
    "        slang=slang.split('\\n') \n",
    "          \n",
    "        tweet_tokens= corpus_review[i].split() \n",
    "        slang_word=[] \n",
    "        meaning=[] \n",
    "          \n",
    "        for line in slang: \n",
    "            temp=line.split(\"=\") \n",
    "            slang_word.append(temp[0]) \n",
    "            meaning.append(temp[-1]) \n",
    "          \n",
    "        for i,word in enumerate(tweet_tokens): \n",
    "            if word in slang_word: \n",
    "                idx=slang_word.index(word) \n",
    "                tweet_tokens[i]=meaning[idx] \n",
    "                  \n",
    "        corpus_review[i]=\" \".join(tweet_tokens) \n",
    "        corpus_review[i] = ''.join(''.join(s)[:2] for _, s in itertools.groupby(corpus_review[i]))   \n",
    " \n",
    "        spell = Speller(lang='en') \n",
    "        corpus_review[i]=spell(corpus_review[i]) \n",
    "    return corpus_review\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se extrae en un corpus todos los reviews o criticas de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus(dataset):\n",
    "    lista = []  \n",
    "    for i in range(len(dataset['package_name'].unique())):\n",
    "        dataset_temp = dataset.loc[dataset['package_name'] == dataset['package_name'].unique()[i]]\n",
    "        lista.append({'package_name':dataset['package_name'].unique()[i], 'size': len(dataset_temp)})\n",
    "\n",
    "    lista = sorted(lista, key=lambda x: x['size'], reverse=True)\n",
    "    dataframe = dataset[dataset['package_name'] == lista[8]['package_name']]\n",
    "    corpus = list(dataframe['review'])\n",
    "    \n",
    "    #corpus = clean_corpus(corpus)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_corpus_to_dataFrame(corpus):\n",
    "    corpus_ds = {\n",
    "        'Sentences' : corpus\n",
    "    }\n",
    "\n",
    "    dataset_new = pd.DataFrame(corpus_ds)\n",
    "    return dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una funcion que nos permita hacer una incrustacion de palabras con un dataset dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_embeddings(dataset):\n",
    "    model_embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    corpus = extract_corpus(dataset)\n",
    "\n",
    "    embeddings = model_embedder.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n",
    "    embeddings = embeddings /  np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    return embeddings, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_reduction(embeddings):\n",
    "    scaler = umap.UMAP(n_components=2).fit_transform(embeddings)\n",
    "    dimension_2d = pd.DataFrame(scaler, columns=['x', 'y'])\n",
    "    return dimension_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos tambien la funcion para el metodo de la silueta, este recibe dos argumentos: el dataset y el numero de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhoutte(dataset, attempts):\n",
    "\n",
    "    embeddings, corpus = neural_embeddings(dataset)\n",
    "    scores_silhouette = []\n",
    "\n",
    "    for k in range(2,attempts):\n",
    "\n",
    "        agglomerative_clusterering = AgglomerativeClustering(n_clusters=k, affinity=\"cosine\" , linkage=\"complete\").fit(embeddings)\n",
    "        cluster_labels = agglomerative_clusterering.labels_\n",
    "\n",
    "        silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
    "        scores_silhouette.append(silhouette_avg)\n",
    "\n",
    "    max_score = max(scores_silhouette)\n",
    "    max_index = scores_silhouette.index(max_score)\n",
    "    n_clusters = max_index + 2\n",
    "\n",
    "    return n_clusters, embeddings, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace una funcion para la segmentación del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(dataset_review, attempts):\n",
    "    n_clusters, embeddings, corpus = silhoutte(dataset_review, attempts)\n",
    "\n",
    "    agglomerative_clusterering = AgglomerativeClustering(n_clusters=n_clusters, affinity=\"cosine\" , linkage=\"complete\").fit(embeddings)\n",
    "    cluster_labels = agglomerative_clusterering.labels_\n",
    "\n",
    "    return cluster_labels, embeddings, corpus, n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clustering(dataset_review, attempts):\n",
    "    labels, embeddings, corpus,n_clusters = segmentation(dataset_review, attempts)\n",
    "    \n",
    "    corpus_dataframe = convert_corpus_to_dataFrame(corpus)\n",
    "    corpus_dataframe['cluster'] = labels\n",
    "    #corpus_dataframe = corpus_dataframe.sort_values(by=['cluster'])\n",
    "\n",
    "    return labels, embeddings, corpus_dataframe,n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Topics(df, xtopic):\n",
    "    docs = df.to_list()\n",
    "    # create model \n",
    "    model = BERTopic(nr_topics=xtopic,verbose=True).fit(docs)\n",
    "    #convert to list \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graphics_and_themes(dataset_review, attemps):\n",
    "\n",
    "    labels_HAC, embeddings, segment_review, n_clusters = clustering(dataset_review, attemps)\n",
    "    model=Topics(segment_review['Sentences'],n_clusters)\n",
    "    review_2d = dimension_reduction(embeddings)\n",
    "\n",
    "    review_2d['labels'] = labels_HAC\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    clustered = review_2d[review_2d.labels != -1]\n",
    "    plt.scatter(review_2d.x, review_2d.y, c=clustered.labels, s=20, cmap='Spectral')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    model.visualize_barchart()\n",
    "    return segment_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 93/93 [00:40<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "print(show_graphics_and_themes(review, 40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110fe3fb9777db4ce1f884af3cc527a40b2c98427ad17781c021ef692bd3d28d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
