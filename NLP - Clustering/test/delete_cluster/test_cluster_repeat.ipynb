{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "from bertopic import BERTopic\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura del dataset \"app_reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>package_name</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Great app! The new version now works on my Bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Great It's not fully optimised and has some is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Works on a Nexus 6p I'm still messing around w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>The bandwidth seemed to be limited to maximum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Works well with my Hackrf Hopefully new update...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288060</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>it doesn't do anything after installing this i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288061</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>I like this app . Its is very helpful for use....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288062</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>Finally Brings back the Unix command line to A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288063</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>The API feature is great  just need loads more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288064</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>Works Nicely! I wish there were instructions t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288065 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   package_name  \\\n",
       "0       com.mantz_it.rfanalyzer   \n",
       "1       com.mantz_it.rfanalyzer   \n",
       "2       com.mantz_it.rfanalyzer   \n",
       "3       com.mantz_it.rfanalyzer   \n",
       "4       com.mantz_it.rfanalyzer   \n",
       "...                         ...   \n",
       "288060           com.termux.api   \n",
       "288061           com.termux.api   \n",
       "288062           com.termux.api   \n",
       "288063           com.termux.api   \n",
       "288064           com.termux.api   \n",
       "\n",
       "                                                   review  \n",
       "0       Great app! The new version now works on my Bra...  \n",
       "1       Great It's not fully optimised and has some is...  \n",
       "2       Works on a Nexus 6p I'm still messing around w...  \n",
       "3       The bandwidth seemed to be limited to maximum ...  \n",
       "4       Works well with my Hackrf Hopefully new update...  \n",
       "...                                                   ...  \n",
       "288060  it doesn't do anything after installing this i...  \n",
       "288061  I like this app . Its is very helpful for use....  \n",
       "288062  Finally Brings back the Unix command line to A...  \n",
       "288063  The API feature is great  just need loads more...  \n",
       "288064  Works Nicely! I wish there were instructions t...  \n",
       "\n",
       "[288065 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = pd.read_csv('https://raw.githubusercontent.com/LuisSante/Datasets/main/app_reviews.csv')\n",
    "review = review.drop(['date','star'],axis=1)\n",
    "#review = pd.read_csv('C:/Users/USUARIO/Documents/Universidad/4A. Inteligencia Artificial/Dataset/app_reviews.csv')\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se extrae en un corpus todos los reviews o criticas de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus(dataset):\n",
    "    print(datetime.today(), \"extrayendo oraciones...\")\n",
    "    lista = []  \n",
    "    for i in range(len(dataset['package_name'].unique())):#iterar entre los package_name unicos\n",
    "        dataset_temp = dataset.loc[dataset['package_name'] == dataset['package_name'].unique()[i]]\n",
    "        lista.append({'package_name':dataset['package_name'].unique()[i], 'size': len(dataset_temp)})#otener un package_name y el número de oraciones\n",
    "\n",
    "    lista = sorted(lista, key=lambda x: x['size'], reverse=True)#se ordena para saber que package_name tiene el mayor n° de oraciones\n",
    "    dataframe = dataset[dataset['package_name'] == lista[8]['package_name']]#el mayor será el elemnto que ocupa la posicion 0\n",
    "    corpus = list(dataframe['review'])#extraemos un corpus\n",
    "    \n",
    "    #corpus = clean_corpus(corpus)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir datos en un Dataframe a un manejo más ágil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_corpus_to_dataFrame(corpus):\n",
    "    print(datetime.today(), \"Convirtiendo las oraciones extraidas a un dataframe...\")\n",
    "    corpus_ds = {\n",
    "        'Sentences' : corpus\n",
    "    }\n",
    "\n",
    "    dataset_new = pd.DataFrame(corpus_ds)\n",
    "    return dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_embbedings_to_dataFrame(embeddings):\n",
    "    print(datetime.today(), \"Convirtiendo las incrustaciones a un dataframe...\")\n",
    "    array = []\n",
    "    for i in embeddings:\n",
    "        array.append([i])\n",
    "\n",
    "    dataset_new = pd.DataFrame(array, columns=['Embeddings'])\n",
    "    return dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea una función que nos permita incrustar las oraciones, para esto usamos un modelo pre-entrenado de SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_embeddings(dataset):\n",
    "    model_embedder = SentenceTransformer('all-MiniLM-L6-v2')#modelo pre-entrenado\n",
    "    corpus = extract_corpus(dataset)#extraemos un corpus del dataset \n",
    "    print(datetime.today(), \"Incrustando las oraciones...\")\n",
    "    embeddings = model_embedder.encode(corpus, \n",
    "                                        convert_to_tensor=False, \n",
    "                                        show_progress_bar=True) #generamos las incrustaciones \n",
    "\n",
    "    embeddings = embeddings /  np.linalg.norm(embeddings, axis=1, keepdims=True) #normalizamos\n",
    "\n",
    "    return embeddings, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_embeddings_queries(queries):\n",
    "    model_embedder = SentenceTransformer('all-MiniLM-L6-v2')#modelo pre-entrenado\n",
    "\n",
    "    embeddings_queries = model_embedder.encode(queries, \n",
    "                                        convert_to_tensor=False) #generamos las incrustaciones \n",
    "\n",
    "    embeddings_queries = embeddings_queries /  np.linalg.norm(embeddings_queries, axis=0, keepdims=True) #normalizamos\n",
    "\n",
    "    return embeddings_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para obtener el \"mejor\" cluster aplicamos el método de la silueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en esta funcion hace la tarea de obtener el mejor k con agglomerative clustering\n",
    "def silhoutte(dataset, attempts):\n",
    "    embeddings, corpus = neural_embeddings(dataset)\n",
    "    print(datetime.today(), \"Calculando el mejor k...\")\n",
    "    scores_silhouette = [] #guardaremos todos los resultados del método de la silueta para devolver el mayor\n",
    "\n",
    "    for k in range(2,attempts+1):\n",
    "        agglomerative_clusterering = AgglomerativeClustering(n_clusters=k, \n",
    "                                                            affinity=\"cosine\" , \n",
    "                                                            linkage=\"complete\").fit(embeddings)\n",
    "                                                            \n",
    "        cluster_labels = agglomerative_clusterering.labels_\n",
    "\n",
    "        silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
    "        scores_silhouette.append(silhouette_avg)\n",
    "\n",
    "    max_score = max(scores_silhouette)\n",
    "    max_index = scores_silhouette.index(max_score)\n",
    "    n_clusters = max_index + 2\n",
    "\n",
    "    return n_clusters, embeddings, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Al tener el \"mejor\" número de clusters, se procede a segmentar las oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_segmentation(dataset_review, attempts):\n",
    "    n_clusters, embeddings, corpus = silhoutte(dataset_review, attempts) # se le pasa el mejor K\n",
    "\n",
    "    agglomerative_clusterering = AgglomerativeClustering(n_clusters=n_clusters, \n",
    "                                                        affinity=\"cosine\", \n",
    "                                                        linkage=\"complete\").fit(embeddings)\n",
    "                                                        \n",
    "    cluster_labels = agglomerative_clusterering.labels_ #obtengo las etiquetas respectivas a las oraciones\n",
    "\n",
    "    model_topics = BERTopic(nr_topics = n_clusters, language='english') # entreno para sacar K temas \n",
    "    topics, prob = model_topics.fit_transform(corpus)\n",
    "\n",
    "    label_topics = model_topics.generate_topic_labels(nr_words=5, topic_prefix=False) # temas\n",
    "\n",
    "    label_topics.pop(0) #elimino el grupo de temas atípicos\n",
    "\n",
    "    return cluster_labels, label_topics, embeddings, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A cada oración le asignamos el cluster al que pertenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(dataset_review, attempts):\n",
    "    cluster_labels, label_topics, embeddings, corpus = topics_segmentation(dataset_review, attempts)\n",
    "    print(datetime.today(), \"Asignando un cluster a cada oración...\")\n",
    "    corpus_dataframe = convert_corpus_to_dataFrame(corpus) #de set de oraciones se convierte en un DF para asignarle su número de cluster\n",
    "    corpus_dataframe['cluster'] = cluster_labels #se le asigna a cada oración un cluster\n",
    "\n",
    "    return embeddings, label_topics, corpus_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda semántica para encontrar el tema de cada cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(dataset_review, attemps):\n",
    "    embeddings, label_topics, corpus_dataframe = clustering(dataset_review, attemps) #tomo embeddings para no volver a hacer el mismo trabajo 2 veces\n",
    "    dataframe_embeddings = convert_embbedings_to_dataFrame(embeddings) #convierto a cada lista de embeddings en un DF para evaluar con cada tema\n",
    "    dataframe_embeddings['cluster'] = corpus_dataframe['cluster'] # le asigno los clusters\n",
    "    sort_embeddings =  dataframe_embeddings.sort_values(by=['cluster']) \n",
    "    sort_embeddings = sort_embeddings.reset_index(drop=True)\n",
    "    nr_clusters = sort_embeddings['cluster'].unique() # extrae las un representante de cada cluster\n",
    "\n",
    "    first_sentences = [] #se almacenara cada primera oracion incrustada de cada cluster para asignarle un topic\n",
    "    j = 0\n",
    "    i = 0\n",
    "    while i < len(sort_embeddings):               \n",
    "        if(j < len(nr_clusters) and sort_embeddings['cluster'][i] == nr_clusters[j]):\n",
    "            first_sentences.append(sort_embeddings['Embeddings'][i]) #almacena\n",
    "            j+=1\n",
    "        i+=1\n",
    "\n",
    "    queries = label_topics #queries seran los temas\n",
    "    topics = [] \n",
    "    in_clusters = [] #se almacena los temas y los clusters, para que tengan un mismo índice en común\n",
    "    print(datetime.today(), \"Incrustando los temas...\")\n",
    "    for topic in queries:\n",
    "\n",
    "        embeddings_queries = neural_embeddings_queries(topic) #incrusta los temas \n",
    "        cos_scores = util.cos_sim(embeddings_queries, first_sentences)[0] #se saca la similaridad de cada tema con respecto a las demas oraciones\n",
    "\n",
    "        cos_scores_numpy = cos_scores.numpy() #se convierte a tensor a numpu\n",
    "        cos_scores_list = cos_scores_numpy.tolist() #se convierte de numpy a list\n",
    "        max_coincidence = max(cos_scores_list)\n",
    "        cluster = cos_scores_list.index(max_coincidence)\n",
    "\n",
    "        topics.append(topic)\n",
    "        in_clusters.append(cluster)\n",
    "    \n",
    "    \n",
    "    tupla = [] ##tamaño k -> k es el tamaño de cluster\n",
    "    for i in range(len(topics)):\n",
    "        tupla.append({'Topics': topics[i] , 'Cluster': in_clusters[i]}) #empareja    \n",
    "\n",
    "    return tupla\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tupla = semantic_search(review, 40)\n",
    "tupla = pd.DataFrame(tupla, columns=['Topics','Cluster'])\n",
    "tupla = tupla.sort_values(by=['Cluster'])\n",
    "tupla = tupla.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tupla"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110fe3fb9777db4ce1f884af3cc527a40b2c98427ad17781c021ef692bd3d28d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
