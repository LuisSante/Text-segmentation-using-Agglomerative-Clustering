{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.preprocessing import normalize, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding, SpectralEmbedding\n",
    "from sklearn.decomposition import KernelPCA, SparsePCA, TruncatedSVD, PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import re     \n",
    "import itertools \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>package_name</th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Great app! The new version now works on my Bra...</td>\n",
       "      <td>October 12 2016</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Great It's not fully optimised and has some is...</td>\n",
       "      <td>August 23 2016</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Works on a Nexus 6p I'm still messing around w...</td>\n",
       "      <td>August 04 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>The bandwidth seemed to be limited to maximum ...</td>\n",
       "      <td>July 25 2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>com.mantz_it.rfanalyzer</td>\n",
       "      <td>Works well with my Hackrf Hopefully new update...</td>\n",
       "      <td>July 22 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288060</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>it doesn't do anything after installing this i...</td>\n",
       "      <td>June 24 2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288061</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>I like this app . Its is very helpful for use....</td>\n",
       "      <td>June 20 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288062</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>Finally Brings back the Unix command line to A...</td>\n",
       "      <td>May 20 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288063</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>The API feature is great  just need loads more...</td>\n",
       "      <td>May 05 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288064</th>\n",
       "      <td>com.termux.api</td>\n",
       "      <td>Works Nicely! I wish there were instructions t...</td>\n",
       "      <td>April 28 2016</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288065 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   package_name  \\\n",
       "0       com.mantz_it.rfanalyzer   \n",
       "1       com.mantz_it.rfanalyzer   \n",
       "2       com.mantz_it.rfanalyzer   \n",
       "3       com.mantz_it.rfanalyzer   \n",
       "4       com.mantz_it.rfanalyzer   \n",
       "...                         ...   \n",
       "288060           com.termux.api   \n",
       "288061           com.termux.api   \n",
       "288062           com.termux.api   \n",
       "288063           com.termux.api   \n",
       "288064           com.termux.api   \n",
       "\n",
       "                                                   review             date  \\\n",
       "0       Great app! The new version now works on my Bra...  October 12 2016   \n",
       "1       Great It's not fully optimised and has some is...   August 23 2016   \n",
       "2       Works on a Nexus 6p I'm still messing around w...   August 04 2016   \n",
       "3       The bandwidth seemed to be limited to maximum ...     July 25 2016   \n",
       "4       Works well with my Hackrf Hopefully new update...     July 22 2016   \n",
       "...                                                   ...              ...   \n",
       "288060  it doesn't do anything after installing this i...     June 24 2016   \n",
       "288061  I like this app . Its is very helpful for use....     June 20 2016   \n",
       "288062  Finally Brings back the Unix command line to A...      May 20 2016   \n",
       "288063  The API feature is great  just need loads more...      May 05 2016   \n",
       "288064  Works Nicely! I wish there were instructions t...    April 28 2016   \n",
       "\n",
       "        star  \n",
       "0          4  \n",
       "1          4  \n",
       "2          5  \n",
       "3          3  \n",
       "4          5  \n",
       "...      ...  \n",
       "288060     3  \n",
       "288061     5  \n",
       "288062     5  \n",
       "288063     5  \n",
       "288064     5  \n",
       "\n",
       "[288065 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = pd.read_csv('https://raw.githubusercontent.com/LuisSante/Datasets/main/app_reviews.csv')\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus(dataset):\n",
    "    lista = []  \n",
    "    for i in range(len(dataset['package_name'].unique())):\n",
    "        dataset_temp = dataset.loc[dataset['package_name'] == dataset['package_name'].unique()[i]]\n",
    "        lista.append({'package_name':dataset['package_name'].unique()[i], 'size': len(dataset_temp)})\n",
    "\n",
    "    lista = sorted(lista, key=lambda x: x['size'], reverse=True)\n",
    "    dataframe = dataset[dataset['package_name'] == lista[8]['package_name']]\n",
    "    corpus = list(dataframe['review'])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_corpus_to_dataFrame(corpus):\n",
    "    corpus_ds = {\n",
    "        'Sentences' : corpus\n",
    "    }\n",
    "\n",
    "    dataset_new = pd.DataFrame(corpus_ds)\n",
    "    return dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(ds_new):\n",
    "        \n",
    "    for i in range(1094):\n",
    "        sentences=ds_new.Sentences[i]\n",
    "       # print(sentences)\n",
    "        \n",
    "        ds_new.Sentences[i] = re.sub(r'https?:\\/\\/.\\S+', \"\", ds_new.Sentences[i]) \n",
    "          \n",
    "        ds_new.Sentences[i] = re.sub(r'\"', '', ds_new.Sentences[i]) \n",
    "        \n",
    "        ds_new.Sentences[i] = re.sub(r'#', '', ds_new.Sentences[i]) \n",
    "          \n",
    "        ds_new.Sentences[i] = re.sub(r'^RT[\\s]+', '', ds_new.Sentences[i]) \n",
    "              \n",
    "        Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'    ll\":\" will\", \n",
    "               \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}     \n",
    "          \n",
    "        for key,value in Apos_dict.items(): \n",
    "            if key in ds_new.Sentences[i]: \n",
    "                ds_new.Sentences[i]=ds_new.Sentences[i].replace(key,value) \n",
    "        ds_new.Sentences[i] = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",ds_new.Sentences[i]) if s])\n",
    "        ds_new.Sentences[i]=ds_new.Sentences[i].lower() \n",
    "        file=open(\"slang.txt\",\"r\") \n",
    "        slang=file.read() \n",
    "          \n",
    "        slang=slang.split('\\n') \n",
    "          \n",
    "        tweet_tokens= ds_new.Sentences[i].split() \n",
    "        slang_word=[] \n",
    "        meaning=[] \n",
    "          \n",
    "        for line in slang: \n",
    "            temp=line.split(\"=\") \n",
    "            slang_word.append(temp[0]) \n",
    "            meaning.append(temp[-1]) \n",
    "          \n",
    "        for i,word in enumerate(tweet_tokens): \n",
    "            if word in slang_word: \n",
    "                idx=slang_word.index(word) \n",
    "                tweet_tokens[i]=meaning[idx] \n",
    "                  \n",
    "        ds_new.Sentences[i]=\" \".join(tweet_tokens) \n",
    "        ds_new.Sentences[i] = ''.join(''.join(s)[:2] for _, s in itertools.groupby(ds_new.Sentences[i]))   \n",
    "        from autocorrect import Speller  \n",
    "        spell = Speller(lang='en') \n",
    "        ds_new.Sentences[i]=spell(ds_new.Sentences[i]) \n",
    "    return ds_new\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_embeddings(dataset):\n",
    "    model_embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    corpus_ = extract_corpus(dataset)\n",
    "    corpus = clean_corpus(corpus_) \n",
    "    corpus_embeddings = model_embedder.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n",
    "    corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    return corpus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhoutte(dataset, attempts):\n",
    "\n",
    "    embeddings = neural_embeddings(dataset)\n",
    "    scores_silhouette = []\n",
    "\n",
    "    for k in range(2,attempts):\n",
    "\n",
    "        agglomerative_clusterering = AgglomerativeClustering(n_clusters=k, affinity=\"cosine\" , linkage=\"complete\").fit(embeddings)\n",
    "        cluster_labels = agglomerative_clusterering.labels_\n",
    "\n",
    "        silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
    "        scores_silhouette.append(silhouette_avg)\n",
    "\n",
    "    max_score = max(scores_silhouette)\n",
    "    max_index = scores_silhouette.index(max_score)\n",
    "    n_clusters = max_index + 2\n",
    "\n",
    "    return n_clusters, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(dataset_review, attempts):\n",
    "    n_clusters, embeddings = silhoutte(dataset_review, attempts)\n",
    "\n",
    "    agglomerative_clusterering = AgglomerativeClustering(n_clusters=n_clusters, affinity=\"cosine\" , linkage=\"complete\").fit(embeddings)\n",
    "    cluster_labels = agglomerative_clusterering.labels_\n",
    "\n",
    "    return n_clusters, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clustering(dataset_review, attempts):\n",
    "    n_clusters, labels = segmentation(dataset_review, attempts)\n",
    "    corpus_dataset = extract_corpus(dataset_review)\n",
    "\n",
    "    clustered_sentences = {}\n",
    "    for sentence_id, cluster_id in enumerate(labels):\n",
    "        if cluster_id not in clustered_sentences:\n",
    "            clustered_sentences[cluster_id] = []\n",
    "    \n",
    "        clustered_sentences[cluster_id].append(corpus_dataset[sentence_id])\n",
    "    \n",
    "    for i, cluster in clustered_sentences.items():\n",
    "        print(\"Cluster \", i+1)\n",
    "        print(cluster)\n",
    "        print(\"     \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering(review, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_dim(corpus_embeddings):\n",
    "    scaler = PCA(n_components=100, random_state = 100)\n",
    "    X_principal = scaler.fit_transform(corpus_embeddings)\n",
    "    X_principal = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=3).fit_transform(X_principal)\n",
    "    distribution = pd.DataFrame(X_principal, columns=['x', 'y'])\n",
    "    distribution\n",
    "    \n",
    "    return X_principal, distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dimentions(distribution):\n",
    "    for col in 'xy':\n",
    "        sns.kdeplot(distribution[col], shade=True)\n",
    "\n",
    "    with sns.axes_style(style='ticks'):\n",
    "       g = sns.factorplot(data=distribution, kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graphics(corpus_embeddings,X_principal,labels_):\n",
    "    point_size = 100.0 / np.sqrt(corpus_embeddings.shape[0])\n",
    "    result = pd.DataFrame(X_principal, columns=['x', 'y'])\n",
    "    result['labels'] = labels_\n",
    "    print(result)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    clustered = result[result.labels != -1]\n",
    "    plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=20, cmap='Spectral')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_whit_m(dataset_review, attempts):\n",
    "    x_principal, distribution=red_dim(dataset_review)\n",
    "    show_dimentions(distribution)\n",
    "\n",
    "    n_clusters, labels = segmentation(x_principal, attempts)\n",
    "    corpus_dataset = extract_corpus(x_principal)\n",
    "\n",
    "    clustered_sentences = {}\n",
    "    for sentence_id, cluster_id in enumerate(labels):\n",
    "        if cluster_id not in clustered_sentences:\n",
    "            clustered_sentences[cluster_id] = []\n",
    "    \n",
    "        clustered_sentences[cluster_id].append(corpus_dataset[sentence_id])\n",
    "    \n",
    "    for i, cluster in clustered_sentences.items():\n",
    "        print(\"Cluster \", i+1)\n",
    "        print(cluster)\n",
    "        print(\"     \")\n",
    "    show_graphics(corpus_dataset,x_principal,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'com.mantz_it.rfanalyzer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marit\\Documents\\UNSA\\ICC\\Text-segmentation-using-Agglomerative-Clustering\\NLP - Clustering\\exp_clusters_2.ipynb Celda 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/marit/Documents/UNSA/ICC/Text-segmentation-using-Agglomerative-Clustering/NLP%20-%20Clustering/exp_clusters_2.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m clustering_whit_m(review, \u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\marit\\Documents\\UNSA\\ICC\\Text-segmentation-using-Agglomerative-Clustering\\NLP - Clustering\\exp_clusters_2.ipynb Celda 15\u001b[0m in \u001b[0;36mclustering_whit_m\u001b[1;34m(dataset_review, attempts)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marit/Documents/UNSA/ICC/Text-segmentation-using-Agglomerative-Clustering/NLP%20-%20Clustering/exp_clusters_2.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclustering_whit_m\u001b[39m(dataset_review, attempts):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/marit/Documents/UNSA/ICC/Text-segmentation-using-Agglomerative-Clustering/NLP%20-%20Clustering/exp_clusters_2.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     x_principal, distribution\u001b[39m=\u001b[39mred_dim(dataset_review)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marit/Documents/UNSA/ICC/Text-segmentation-using-Agglomerative-Clustering/NLP%20-%20Clustering/exp_clusters_2.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     show_dimentions(distribution)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marit/Documents/UNSA/ICC/Text-segmentation-using-Agglomerative-Clustering/NLP%20-%20Clustering/exp_clusters_2.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     n_clusters, labels \u001b[39m=\u001b[39m segmentation(x_principal, attempts)\n",
      "\u001b[1;32mc:\\Users\\marit\\Documents\\UNSA\\ICC\\Text-segmentation-using-Agglomerative-Clustering\\NLP - Clustering\\exp_clusters_2.ipynb Celda 15\u001b[0m in \u001b[0;36mred_dim\u001b[1;34m(corpus_embeddings)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marit/Documents/UNSA/ICC/Text-segmentation-using-Agglomerative-Clustering/NLP%20-%20Clustering/exp_clusters_2.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mred_dim\u001b[39m(corpus_embeddings):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marit/Documents/UNSA/ICC/Text-segmentation-using-Agglomerative-Clustering/NLP%20-%20Clustering/exp_clusters_2.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     scaler \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/marit/Documents/UNSA/ICC/Text-segmentation-using-Agglomerative-Clustering/NLP%20-%20Clustering/exp_clusters_2.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     X_principal \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(corpus_embeddings)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marit/Documents/UNSA/ICC/Text-segmentation-using-Agglomerative-Clustering/NLP%20-%20Clustering/exp_clusters_2.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     X_principal \u001b[39m=\u001b[39m TSNE(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m,init\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrandom\u001b[39m\u001b[39m'\u001b[39m, perplexity\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mfit_transform(X_principal)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marit/Documents/UNSA/ICC/Text-segmentation-using-Agglomerative-Clustering/NLP%20-%20Clustering/exp_clusters_2.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     distribution \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X_principal, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\decomposition\\_pca.py:433\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    412\u001b[0m     \u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \n\u001b[0;32m    414\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m     U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m    434\u001b[0m     U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[0;32m    436\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[0;32m    437\u001b[0m         \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\decomposition\\_pca.py:456\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    451\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPCA does not support sparse input. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTruncatedSVD for a possible alternative.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m     )\n\u001b[1;32m--> 456\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    457\u001b[0m     X, dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32], ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    458\u001b[0m )\n\u001b[0;32m    460\u001b[0m \u001b[39m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'com.mantz_it.rfanalyzer'"
     ]
    }
   ],
   "source": [
    "clustering_whit_m(review, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110fe3fb9777db4ce1f884af3cc527a40b2c98427ad17781c021ef692bd3d28d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
